{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs *generate* text\n",
    "\n",
    "**A really smart autocomplete, trained on billions of examples!**\n",
    "\n",
    "What it is actually doing: is predicting the most statistically likely next word.\n",
    "\n",
    "Uses probability distributions over its vocabulary to predict/decide what comes next.\n",
    "\n",
    "For example, if you type **\"I need a cup of\"**, the model might assign:\n",
    "- P(\"coffee\") = 0.75\n",
    "- P(\"tea\") = 0.15\n",
    "- P(\"water\") = 0.05\n",
    "- P(\"sunshine\") = 0.01\n",
    "\n",
    "**Coffee** seems to be the most likely next word.\n",
    "\n",
    "Conditional probability of the next word given the previous words.\n",
    "\n",
    "The more context the model sees, the better it gets at choosing relevant words. -- **Is this the Attention Part?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "The **tiktoken** library comes with access to a precompiled vocabulary and merge rules.\n",
    "\n",
    "These are effectively the token database used by models like GPT-3.5 and GPT-4.\n",
    "\n",
    "### Token Vocabulary\n",
    "- A list of all valid tokens (words, subwords, symbols, etc.) and their corresponding token IDs.\n",
    "- For **GPT-4**, this is around **~100,264 tokens**.\n",
    "\n",
    "### Merge Rules (Byte Pair Encoding)\n",
    "- A ranked list of the most common token pairs seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 'Can' -> Token ID: 6854\n",
      "2: ' you' -> Token ID: 499\n",
      "3: ' explain' -> Token ID: 10552\n",
      "4: ' what' -> Token ID: 1148\n",
      "5: ' a' -> Token ID: 264\n",
      "6: ' lymph' -> Token ID: 43745\n",
      "7: 'atic' -> Token ID: 780\n",
      "8: ' system' -> Token ID: 1887\n",
      "9: ' is' -> Token ID: 374\n",
      "10: '?' -> Token ID: 30\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load the tokenizer for GPT-4 or GPT-3.5\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")  # or \"gpt-3.5-turbo\"\n",
    "\n",
    "text = \"Can you explain what a lymphatic system is?\"\n",
    "\n",
    "tokens = enc.encode(text)\n",
    "token_strings = [enc.decode([token]) for token in tokens]\n",
    "\n",
    "# Print tokens\n",
    "for i, (token_id, token_str) in enumerate(zip(tokens, token_strings)):\n",
    "    print(f\"{i+1}: {token_str!r} -> Token ID: {token_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leading Spaces\n",
    "\n",
    "GPT models are trained based on token sequences i.e. not word sequences.\n",
    "\n",
    "Be cognizant of spaces as it **impacts** token count\n",
    "- could map to different token ids\n",
    "- will affect costs and limits\n",
    "\n",
    "Important when prompting\n",
    "- Proper spacing matters in prompts. If you forget a leading space in a sentence, GPT may misinterpret the prompt, or split words oddly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"antidisestablishmentarianism\"\n",
      "[519, 85342, 34500, 479, 8997, 2191]\n",
      "\n",
      "\" antidisestablishmentarianism\"\n",
      "[3276, 85342, 34500, 479, 8997, 2191]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# word = \"fantastic\"\n",
    "# word = \"banana\"\n",
    "# word = \"hello\"\n",
    "# word = \"  hello\"\n",
    "word = \"antidisestablishmentarianism\"\n",
    "\n",
    "print('\"' + enc.decode(enc.encode(word)) + '\"')\n",
    "print(enc.encode(word))  # [token_id]\n",
    "print()\n",
    "print('\"' + enc.decode(enc.encode(\" \" + word)) + '\"')\n",
    "print(enc.encode(\" \" + word))   # Might be different or split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Happens\n",
    "\n",
    "### Spaces Can Carry Meaning\n",
    "In natural language, spaces aren’t always just separators. Consider:\n",
    "- Indentation in code\n",
    "- Formatting in poetry or screenplays\n",
    "- Double spaces after a period (some people still do this!)\n",
    "\n",
    "### Byte Pair Encoding (BPE) tries to:\n",
    "- Find the longest possible matches in its vocabulary.\n",
    "- Prioritize common phrases and words (with leading spaces) as single tokens.\n",
    "\n",
    "The model learns to interpret text as it appears — not a cleaned-up version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'!' -> Token ID: 0\n",
      "'\"' -> Token ID: 1\n",
      "'#' -> Token ID: 2\n",
      "'$' -> Token ID: 3\n",
      "'%' -> Token ID: 4\n",
      "'&' -> Token ID: 5\n",
      "\"'\" -> Token ID: 6\n",
      "'(' -> Token ID: 7\n",
      "')' -> Token ID: 8\n",
      "'*' -> Token ID: 9\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "#vocab = enc._special_tokens  # Contains special tokens\n",
    "all_tokens = enc._mergeable_ranks  # Dictionary of tokens and their ranks\n",
    "\n",
    "# Print a few tokens\n",
    "for token_bytes, rank in list(all_tokens.items())[:10]:\n",
    "    print(f\"{enc.decode([rank])!r} -> Token ID: {rank}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
